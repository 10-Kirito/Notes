# Linux当中的网络系统

# 0. select/poll/epoll学习

> https://xiaolincoding.com/os/8_network_system/selete_poll_epoll.html#%E6%9C%80%E5%9F%BA%E6%9C%AC%E7%9A%84-socket-%E6%A8%A1%E5%9E%8B
>
> https://zhuanlan.zhihu.com/p/494183512

最基本的socket编程就不说了，就是简单的端对端建立一个连接，进行通信。关键在于如何服务更多的用户呢？

我们可以考虑操作系统当中的进程或者线程，但是如果这样，我们每次新到来一个连接，就需要分配一个进程或者线程，那么如果一台机器要维护1万个连接的话，相当于要维护1万个进程或者线程，对于操作系统来讲，这个是不现实的。

所以，既然为每一个请求分配一个进程/线程的方式不合适，那么有没有可能只使用一个进程来维护多个socket呢？这就是我们的IO多路复用技术。IO多路复用指的是内核一旦发现进程指定的一个或者多个IO条件准备读取，他就通知该进程。

IO多路复用适用于如下场合：

1. 当客户端处理多个描述字的时候(一般是交互式输入和网络套接口)，必须使用IO多路复用；
2. 如果一个TCP服务器既要处理监听端口，又要处理已连接套接口，一般也要使用到IO多路复用；
3. 如果一个服务器既要处理TCP，又要处理UDP，一般要使用IO多路复用；
4. 如果一个服务器要处理多个服务或者多个协议，一般要使用IO多路复用；

与多进程和多线程技术相比，IO多路复用技术的最大优势是系统开销小，系统不需要创建进程/线程，也不必维护这些进程/线程，从而大大的减小了系统的开销。

## 0.1 select

---

*select函数：*

```cpp
#include <sys/select.h>
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

这里讲解一下其中的`select`函数:

->*@nfds*, select函数底层检查对应的socket是否有对应的事件发生的时候使用的是bitmaps，我们需要指定其检测的socket的区间，即为当前建立连接的socket以及服务器端的socket的最大值加一；

->*@readfds*，该参数的类型是`fd_set`该类型就是socket的数组，用来指定是否有可读的socket，传入的readfds是待检测的sockets集合，你可能会疑问上面不是已经指定了范围。对的虽然上面已经指定了范围[0, nfds), 但是其中仍然存在大量的不是我们要判断的sockets，我们需要传入一些socket集合来将那些不是我们目标sockets去除；

->*@writefds*,与*@readfds*一样的作用；

->*@exceptfds*,与*@readfds*一样的作用；

->*@timeout*, 设置的超时时间；

> [!NOTE]
>
> The  timeout  argument  is a timeval structure (shown below) that specifies the interval that select() should block waiting for a file descriptor to become ready.
> The call will block until either:
>
> • a file descriptor becomes ready;
>
> • the call is interrupted by a signal handler; or
>
> • the timeout expires.

---

*File descriptor set相关的函数:*

->*FD_ZERO()*, This macro clears (removes all file descriptors from) set.  It should be employed as the first step in initializing a file descriptor set.

->*FD_SET()*, This macro adds the file descriptor fd to set.  Adding a file descriptor that is already present in the set is a no-op, and does not produce an error.

->*FD_CLR()*, This macro removes the file descriptor fd from set.  Removing a file descriptor that is not present in the set is a no-op, and does not produce an error.

->*FD_ISSET()*, select() modifies the contents of the sets according to the rules described below.  After calling select(), the FD_ISSET() macro can be used to test if a file descriptor is still present in a set.  FD_ISSET() returns nonzero if the file descriptor fd is present in set, and zero if it is not.

---

***下面是一个echo服务器简单展示一下`select`的使用方法：***

[server.c](./code/select-learning/server.c) and [client.c](./code/select-learning/client.c)

**Server端：**

1. 定义结构体`server_context_st`来存储当前服务器端的相关信息，比如说所有的已经建立连接的客户端的socket以及其中最大的socket用来传入`select`函数；

```c
typedef struct server_context_st {
    // the number of clients
    int cli_cnt; 
    // store the file descriptor of clients
    int clidfs[SIZE];
    // the fd we need to monitor
    fd_set allfds;  
    // the max fd
    int maxfd;
}server_context_st;
```

这里的`allfds`是每一次我们调用`select`函数的时候需要设置的变量，我们每一次都要将其清空，之后将服务端的socket以及clidfs数组当中所有的sockets全部存储进来。

```C
static void handle_client_proc(int srvfd) {
    // ...
    fd_set *readfds = &s_srv_ctx->allfds;
    // ...
    int i = 0;
    while (1) {
        // 将服务端的socket添加到readfds当中
        FD_ZERO(readfds);
        FD_SET(srvfd, readfds);
        s_srv_ctx->maxfd = srvfd;
        // ...
        // 将已经建立连接的socket添加到readfds当中
        for(i = 0; i < s_srv_ctx->cli_cnt; i++) {
            clifd = s_srv_ctx->clidfs[i];
            if(clifd != -1){
                FD_SET(clifd, readfds);
            }
            // ...
            s_srv_ctx->maxfd = (clifd > s_srv_ctx->maxfd ? clifd: s_srv_ctx->maxfd);
        }
        // 调用`select`系统函数
        retval = select(s_srv_ctx->maxfd  + 1, readfds, NULL, NULL, &tv);
        // some errors ocured
        if(retval == -1) {
            fprintf(stderr, "select error:%s.\n", strerror(errno));
            return;
        }
        // timeout is over
        if(retval == 0) {
            fprintf(stdout, "select is timeout\n");
            continue;
        }
        // 调用FD_ISSET()函数检查对应的socket是否位于这个集合当中，因为在select函数将会更新这个集合，将其中不可读的socket去掉
        // 只保留符合条件的套接字在这个集合当中, 在一开始初始化readfds的时候，我们都会将srvfd即服务端的socket添加到该集合当中
        // 以及将所有的已经建立连接的客户端socket添加到readfds当中，当select的时候内核会将其中不可读的套接字去掉，只保留符合条件的套接字在这个集合当中.
        if(FD_ISSET(srvfd, readfds)) {
            // 监听客户端请求
            accept_client_proc(srvfd); 
        }else {
            recv_client_msg(readfds);
        }
    }
}
```

这里我们调用`select`之前，对*readfds*进行设置：

```C
static void handle_client_proc(int srvfd) {
  // ...
  fd_set *readfds = &s_srv_ctx->allfds;
  // ...
  while(1) {
    FD_ZERO(readfds);
    FD_SET(srvfd, readfds);
    s_srv_ctx->maxfd = srvfd;
    	
    for(i = 0; i < s_srv_ctx->cli_cnt; i++) {
        clifd = s_srv_ctx->clidfs[i];
        if(clifd != -1){
            FD_SET(clifd, readfds);
        }
        // ...
        s_srv_ctx->maxfd = (clifd > s_srv_ctx->maxfd ? clifd: s_srv_ctx->maxfd);
    }
    // ...
  }
  // ...
}
```

这里就是将所有的已经建立连接的客户端的sockets以及服务器端的socket存储到readfds当中，当我们调用select之后，如果对应的socket没有事件发生的话，内核会对readfds进行修改，将那些没有发生事件的socket删除，也就是说剩下的一定是发生事件的sockets。

接着处理相关的逻辑：

```C
if(FD_ISSET(srvfd, readfds)) {
    // 监听客户端请求
    accept_client_proc(srvfd); 
}else {
  	// 监听客户端消息
    recv_client_msg(readfds);
}
```

这里先对srvfd进行判断，判断其是否仍位于readfds当中，如果位于readfds当中的话，说明有新的客户端连接请求发生，否则的话只能是处理客户端发送的消息。

以上就是简单的`select`的使用场景，我们来看一下这个过程发生了什么？

` select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout)`函数会将readfd，也就是文件描述符集合拷贝到内核当中，让内核来检查是否有网络时间发生，检查的方式很粗暴就是通过遍历文件描述符集合的方式，这里现指定区间[0, nfds), 以及指定readfds、writefds、exceptfds三个集合，来让内核不会错过一些socket或者避免多余的sockets。当有事件产生的时候，将该socket标记为可读或者可写并将其放入到对应的readfds、writefds、exceptfds三个集合当中，那些没有事件发生的sockets，内核则会将其进行一个删除操作。之后再将其拷贝到用户态当中，所以说为什么我们每一次调用`select`的时候需要重新对要传入的文件描述符集合重新进行一个赋值操作。select使⽤固定长度的bitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在Linux系统中，由内核中的FD_SETSIZE限制，默认最⼤值为1024，只能监听0~1023的⽂件描述符。

## 0.2 poll

poll的实现与select非常相似，与select在本质上没有多大的差别，管理多个描述符也是进行*轮询操作*。根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制,其实现使用的是动态数组，以链表形式来进行组织，突破了select的文件描述符个数的限制，当然还会受到系统文件描述符限制。

*二者共同存在的缺点：*包含大量文件描述符的数组被整体复制于用户态和内核的地址空间当中，而不论这些文件描述符是否就绪，他的开销随着文件描述符数量的增加而线性增大。

---

*poll函数：*

```C
#include <poll.h>
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

->*@fds*, poll函数里面不像select那样使用fd_set数据结构，而是使用另外一种数据结构`struct pollfd`:

```C
struct pollfd {	
  int fd; 					// file descriptor
  short events;			// requested events
  short revents;		// returned events
};
```

其中`fd`字段即为相应的连接socket，`events`字段是我们需要监听该socket的事件有哪些?可以这样指定：

```C
cliconnfds[i].events = POLLIN | POLLPRI | POLLOUT；
```

`revents`字段为实际上已经发生的事件，就是内核回去检查对应的socket的events事件是否发生，如果发生了其中一个或者多个，会将其保存在revnets当中返回。

->*@nfds*, 这里和select差不多传入一个需要检查的sockets的上限供其在这个区间进行检查;

->*@timeout*, 设置对应的计时器;

---

***下面是一个echo服务器简单展示一下`poll`的使用方法：***

[server.c](./code/poll-learning/server.c) and [client.c](./code/poll-learning/client.c)

**Server端:**

*main函数：*

```C
int main(int argc, char *argv[]) {
    int listenfd, connfd, sockfd;
    struct sockaddr_in servaddr;
    socklen_t  cliaddrlen;
    listenfd = socket_bind(IPADDRESS, PORT);
    listen(listenfd, LISTENQ);
    do_poll(listenfd);
    return 0;
}
```

就是指定相应的端口和IP进行监听，之后便去调用*do_poll(int sockfd)*函数：

```C
static void do_poll(int listenfd) {
  // ...
  struct pollfd cliendfds[OPEN_MAX];
  int maxi;
  int i;
  int nready;
  // 添加监听描述符
  cliendfds[0].fd = listenfd;
  cliendfds[0].events = POLLIN;
  // 初始化客户端连接描述符
  for(i = 1; i < OPEN_MAX; i++) {
      cliendfds[i].fd = -1;
  }
  maxi = 0;
  for(;;){
    // ...
  }
}
```

该部分初始化相关的参数，添加我们监听的服务端的socket文件描述符，以及将所有的客户端未连接的socket描述符初始化为-1，之后便进入for循环:

```C
for(;;) {
  // 调用poll系统调用，并且返回可用描述符的个数
  nready = poll(cliendfds, maxi + 1, -1);
  if(nready == -1) {
    perror("poll error");
    exit(-1);
  }
  // 测试监听描述符是否有对应的事件发生，即是否有新的客户端连接请求
  if(clientfds[0].revents & POLLIN) {
    // ...
    if(--nready <= 0) {
      continue;
    }
  }
  // 如果仍然存在其他的可用描述符的话，即来自其他的客户端的消息
  handle_connection(clientfds, maxi);
}
```

这里和*select*差不多，都是判断是新的客户端的连接请求还是已经建立连接的客户端发送的消息，并对其进行相应的处理。

**Client端：**

*poll*不仅仅可以监听socket发送的消息，还可以监听本地的输入和输出，这一点也符合我们的常识，因为Linux当中一切皆文件。内核监听socket实质上就是检查对应的文件描述符，那这样的话当然可以检查一下本地的标准输入输出对应的文件描述符。

```C
static void handle_connection(int sockfd) {
    char sendline[MAXLINE], recvline[MAXLINE];
    int maxfdp, stdineof;
    struct pollfd pfds[2];
    int n;
    // 添加连接描述符`
    pfds[0].fd = sockfd;
    pfds[0].events = POLLIN; 
    // 添加标准输入描述符
    pfds[1].fd = STDIN_FILENO;
    pfds[1].events = POLLIN;

    for(;;) {
        poll(pfds, 2, -1);
        if(pfds[0].revents & POLLIN) {
            printf("from server's message\n");
            n = read(sockfd, recvline, MAXLINE);
            if(n == 0){
                fprintf(stderr, "client: server is closed.\n");
                close(sockfd);
            }
            printf("write(STDOUT_FILENOm recvline, n)\n");
            write(STDOUT_FILENO, recvline, n);
        }

        // 测试标准输入是否准备好
        if(pfds[1].revents & POLLIN) {
            n = read(STDIN_FILENO, sendline, MAXLINE);
            if(n == 0) {
                shutdown(sockfd, SHUT_WR);
                continue;
            }
            printf("send the input data to server\n");
            write(sockfd, sendline, n);
        }
    }
}
```

我们可以看到这里使用数组pfds来存储来自服务器端的socket连接以及本地的一个标准输入文件描述符，并监听相应的事件发生。

注意到内核检测标准输入的时候是会陷入阻塞状态的，等待用户输入相关的信息之后，*poll*才会返回处理相应的事件。

---

## 0.3 epoll

epoll通过两个方面，很好解决了select/poll的问题。

->第一点，epoll在内核当中使用红黑树来跟踪进程所有的待检测的文件描述符，把需要监听的socket通过`epoll_ctl()`函数添加到内核中的红黑树当中，红黑树是一个高效的数据结构，增删查改一般的时间复杂度为$O(logn)$。通过对这棵红黑树进行操作，就不需要像select/poll那样每一次操作的时候都传入整个socket集合，只需要传入一个待检测的socket即可，减少了内核和用户空间大量的数据拷贝和内存分配。

->第二点，epoll使用事件驱动的机制，内核当中维护了一个链表来记录就绪事件，当某一个socket有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表当中，当用户调用`epoll_wait()`函数的时候，只会返回有事件发生的文件描述符的个数，***并且修改变量保存相应的就绪事件列表。***不需要像select/poll那样轮询遍历整个socket集合，大大的提高了检测的效率。

---

***下面是一个echo服务器简单展示一下`poll`的使用方法：***

[server.c](./code/epoll-learning/server.c) 

*main函数：*

```C
int main(int argc, char *argv[]) {
    int listenfd;
    listenfd = socket_bind(IPADDRESS, PORT); 
    listen(listenfd, LISTENQ);
    do_epoll(listenfd);
    return 0;
}
```

同样的申请一个socket并进行绑定，之后调用*do_epoll函数*来处理来自客户端的请求。

*do_epoll函数：*

```C
static void do_epoll(int listenfd) {
    int epollfd;
    struct epoll_event events[EPOLLEVENTS];
    int ret;
    char buf[MAXSIZE];
    memset(buf, 0, MAXSIZE);
    epollfd = epoll_create(FDSIZE);

    // 添加监听描述符事件
    add_event(epollfd, listenfd, EPOLLIN);    
    for(;;) {
        // 获取已经准备好的描述符事件
        ret = epoll_wait(epollfd, events, EPOLLEVENTS, -1);
        handle_events(epollfd, events, ret, listenfd, buf);        
    }
    // 当我们结束的时候一定要记得将epoll对应的文件描述符关闭
    close(epollfd); 
}
```

我们在这里调用`epoll_create`来创建一个epoll对应的fd，之后添加监听套接字对应的描述符，之后便进行对应的等待，这里的epoll_wait可以以阻塞以及非阻塞的方式进行调用，具体取决于传递给他的参数`timeout`的值：

- 如果 `timeout` 的值为正数，表示等待指定的毫秒数，当有事件发生或者超时时，`epoll_wait()` 将返回，这是一个非阻塞调用。
- 如果 `timeout` 的值为 `-1`，表示无限期地等待，直到有事件发生为止，这是一个阻塞调用。
- 如果 `timeout` 的值为 `0`，表示立即返回，即检查一次是否有事件发生，但不等待，如果没有事件发生，立即返回，这也是一个非阻塞调用。

返回相应的事件的个数，以及将就绪事件的列表保存在events当中返回，之后对相应的事件进行处理。





# 1. Reactor网络模式

> https://www.xiaolincoding.com/os/8_network_system/reactor.html

一开始的时候软降当中可以做到网络高性能的原因就是因为I/O多路复用，我们利用操作系统所提供的`select`、`poll`和`epoll`等系统调用来书写相应的网络程序。

`select`、`poll`和`epoll`是如何获取网络事件的呢？

在获取事件之前，先把我们要关心的连接传送给内核，再由内核进行检测:

1. 如果没有事件发生的话，线程只需要阻塞在这个系统调用，而无需说我们需要对其进行轮询操作；
2. 如果有事件发生，内核会返回产生了事件的连接，线程就会从阻塞状态返回，然后在用户态当中处理相应的业务即可;

之前的高性能网络模式就是利用这些操作系统提供的系统调用来编写我们的代码，但是这就像使用scoket编写网络程序一样难用，所以说有大佬基于面向对象的思想，对I/O多路复用作了一层封装，让使用者不需要再考虑底层网络API的细节，只需要关注应用代码的编写。

这就是Reactor模式，也就做Dispatcher模式(调度员模式)，也就是IO多路复用监听相应的事件，收到事件之后，根据事件类型分配给某一个进程或者线程。

Reactor模式主要是由Reactor和处理资源池这两个核心部分组成，他俩所负责的事情如下：

1. Reactor负责监听和分发事件，事件类型包含连接事件、读写事件；
2. 处理资源池负责处理事件，比如说read->业务逻辑->send；

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

- Reactor 的数量可以只有一个，也可以有多个；
- 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；

将上面的两个因素排列组设一下，理论上就可以有 4 种方案选择：

- 单 Reactor 单进程 / 线程；
- 单 Reactor 多进程 / 线程；
- 多 Reactor 单进程 / 线程；
- 多 Reactor 多进程 / 线程；

其中，「多 Reactor 单进程 / 线程」实现方案相比「单 Reactor 单进程 / 线程」方案，不仅复杂而且也没有性能优势，因此实际中并没有应用。

剩下的 3 个方案都是比较经典的，且都有应用在实际的项目中：

- 单 Reactor 单进程 / 线程；
- 单 Reactor 多线程 / 进程；
- 多 Reactor 多进程 / 线程；

## 1.1 单Reactor单进程/线程

一般来说，C 语言实现的是「**单 Reactor \*单进程\***」的方案，因为 C 语编写完的程序，运行后就是一个独立的进程，不需要在进程中再创建线程。

而 Java 语言实现的是「**单 Reactor \*单线程\***」的方案，因为 Java 程序是跑在 Java 虚拟机这个进程上面的，虚拟机中有很多线程，我们写的 Java 程序只是其中的一个线程而已。

我们来看看「**单 Reactor 单进程**」的方案示意图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png)

可以看到进程里有 **Reactor、Acceptor、Handler** 这三个对象：

- Reactor 对象的作用是监听和分发事件；
- Acceptor 对象的作用是获取连接；
- Handler 对象的作用是处理业务；

对象里的 select、accept、read、send 是系统调用函数，dispatch 和 「业务处理」是需要完成的操作，其中 dispatch 是分发事件操作。

接下来，介绍下「单 Reactor 单进程」这个方案：

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

单 Reactor 单进程的方案因为全部工作都在同一个进程内完成，所以实现起来比较简单，不需要考虑进程间通信，也不用担心多进程竞争。

但是，这种方案存在 2 个缺点：

- 第一个缺点，因为只有一个进程，**无法充分利用 多核 CPU 的性能**；
- 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

所以，单 Reactor 单进程的方案**不适用计算机密集型的场景，只适用于业务处理非常快速的场景**。

Redis 是由 C 语言实现的，在 Redis 6.0 版本之前采用的正是「单 Reactor 单进程」的方案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的方案。

## 1.2 单Reactor多线程/多进程

如果要克服「单 Reactor 单线程 / 进程」方案的缺点，那么就需要引入多线程 / 多进程，这样就产生了**单 Reactor 多线程 / 多进程**的方案。

闻其名不如看其图，先来看看「单 Reactor 多线程」方案的示意图如下：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png)

详细说一下这个方案：

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；

上面的三个步骤和单 Reactor 单线程方案是一样的，接下来的步骤就开始不一样了：

- Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
- 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。

例如，子线程完成业务处理后，要把结果传递给主线程的 Handler 进行发送，这里涉及共享数据的竞争。

要避免多线程由于竞争共享资源而导致数据错乱的问题，就需要在操作共享资源前加上互斥锁，以保证任意时间里只有一个线程在操作共享资源，待该线程操作完释放互斥锁后，其他线程才有机会操作共享数据。

聊完单 Reactor 多线程的方案，接着来看看单 Reactor 多进程的方案。

事实上，单 Reactor 多进程相比单 Reactor 多线程实现起来很麻烦，主要因为要考虑子进程 <-> 父进程的双向通信，并且父进程还得知道子进程要将数据发送给哪个客户端。

而多线程间可以共享数据，虽然要额外考虑并发问题，但是这远比进程间通信的复杂度低得多，因此实际应用中也看不到单 Reactor 多进程的模式。

另外，「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**。

## 1.3 多Reactor多进程/线程

要解决「单 Reactor」的问题，就是将「单 Reactor」实现成「多 Reactor」，这样就产生了第 **多 Reactor 多进程 / 线程**的方案。

老规矩，闻其名不如看其图。多 Reactor 多进程 / 线程方案的示意图如下（以线程为例）：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png)

方案详细说明如下：

- 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；
- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。
- 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

多 Reactor 多线程的方案虽然看起来复杂的，但是实际实现时比单 Reactor 多线程的方案要简单的多，原因如下：

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理。
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端。

大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。

采用了「多 Reactor 多进程」方案的开源软件是 Nginx，不过方案与标准的多 Reactor 多进程有些差异。

具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接，通过锁来控制一次只有一个子进程进行 accept（防止出现惊群现象），子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程。

# 2. Proactor模式

前面提到的 Reactor 是非阻塞同步网络模式，而 **Proactor 是异步网络模式**。

这里先给大家复习下阻塞、非阻塞、同步、异步 I/O 的概念。

先来看看**阻塞 I/O**，当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。

注意，**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。过程如下图：

![阻塞 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png)

知道了阻塞 I/O ，来看看**非阻塞 I/O**，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。过程如下图：

![非阻塞 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png)

注意，**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**

举个例子，如果 socket 设置了 `O_NONBLOCK` 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。

因此，无论 read 和 send 是阻塞 I/O，还是非阻塞 I/O 都是同步调用。因为在 read 调用时，内核将数据从内核空间拷贝到用户空间的过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。

而真正的**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这**两个过程都不用等待**。

当我们发起 `aio_read` （异步 I/O） 之后，就立即返回，内核自动将数据从内核空间拷贝到用户空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，**应用程序并不需要主动发起拷贝动作**。过程如下图：

![异步 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png)

举个你去饭堂吃饭的例子，你好比应用程序，饭堂好比操作系统。

阻塞 I/O 好比，你去饭堂吃饭，但是饭堂的菜还没做好，然后你就一直在那里等啊等，等了好长一段时间终于等到饭堂阿姨把菜端了出来（数据准备的过程），但是你还得继续等阿姨把菜（内核空间）打到你的饭盒里（用户空间），经历完这两个过程，你才可以离开。

非阻塞 I/O 好比，你去了饭堂，问阿姨菜做好了没有，阿姨告诉你没，你就离开了，过几十分钟，你又来饭堂问阿姨，阿姨说做好了，于是阿姨帮你把菜打到你的饭盒里，这个过程你是得等待的。

异步 I/O 好比，你让饭堂阿姨将菜做好并把菜打到饭盒里后，把饭盒送到你面前，整个过程你都不需要任何等待。

很明显，异步 I/O 比同步 I/O 性能更好，因为异步 I/O 在「内核数据准备好」和「数据从内核空间拷贝到用户空间」这两个过程都不用等待。

Proactor 正是采用了异步 I/O 技术，所以被称为异步网络模型。

现在我们再来理解 Reactor 和 Proactor 的区别，就比较清晰了。

- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。

因此，**Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」**，而 **Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」**。这里的「事件」就是有新连接、有数据可读、有数据可写的这些 I/O 事件这里的「处理」包含从驱动读取到内核以及从内核读取到用户空间。

举个实际生活中的例子，Reactor 模式就是快递员在楼下，给你打电话告诉你快递到你家小区了，你需要自己下楼来拿快递。而在 Proactor 模式下，快递员直接将快递送到你家门口，然后通知你。

无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 **Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件**。

接下来，一起看看 Proactor 模式的示意图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/Proactor.png)

介绍一下 Proactor 模式的工作流程：

- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核；
- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；
- Handler 完成业务处理；

可惜的是，在 Linux 下的异步 I/O 是不完善的， `aio` 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，网络编程中的 socket 是不支持的，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。

而 Windows 里实现了一套完整的支持 socket 的异步编程接口，这套接口就是 `IOCP`，是由操作系统级别实现的异步 I/O，真正意义上异步 I/O，因此在 Windows 里实现高性能网络程序可以使用效率更高的 Proactor 方案。

------

## [#](https://www.xiaolincoding.com/os/8_network_system/reactor.html#总结)总结

常见的 Reactor 实现方案有三种。

第一种方案单 Reactor 单进程 / 线程，不用考虑进程间通信以及数据同步的问题，因此实现起来比较简单，这种方案的缺陷在于无法充分利用多核 CPU，而且处理业务逻辑的时间不能太长，否则会延迟响应，所以不适用于计算机密集型的场景，适用于业务处理快速的场景，比如 Redis（6.0之前 ） 采用的是单 Reactor 单进程的方案。

第二种方案单 Reactor 多线程，通过多线程的方式解决了方案一的缺陷，但它离高并发还差一点距离，差在只有一个 Reactor 对象来承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方。

第三种方案多 Reactor 多进程 / 线程，通过多个 Reactor 来解决了方案二的缺陷，主 Reactor 只负责监听事件，响应事件的工作交给了从 Reactor，Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案，Nginx 则采用了类似于 「多 Reactor 多进程」的方案。

Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」。

因此，真正的大杀器还是 Proactor，它是采用异步 I/O 实现的异步网络模型，感知的是已完成的读写事件，而不需要像 Reactor 感知到事件后，还需要调用 read 来从内核中获取数据。

不过，无论是 Reactor，还是 Proactor，都是一种基于「事件分发」的网络编程模式，区别在于 Reactor 模式是基于「待完成」的 I/O 事件，而 Proactor 模式则是基于「已完成」的 I/O 事件。

# 3. 拓展

## 3.1 Linux系统当中的I/O过程发展

> https://www.xiaolincoding.com/os/8_network_system/

在没有DMA（Direct Memory Access）技术之前，I/O的过程：

<img src="assets/I_O 中断-1713408511620-2.png" alt="I_O 中断" style="zoom:50%;" />

- 首先用户进程发起`read()`系统调用，此时CPU从用户态切换为内核态，之后CPU就会向磁盘发起I/O请求；
- 接着磁盘会将数据放置在磁盘控制器的缓冲区当中，之后产生中断通知CPU；
- CPU收到中断信号之后，会处理该中断，将磁盘控制器的缓冲区的数据一次一个字节读进自己的寄存器当中，接着会再将寄存器当中的数据写入到内存当中（***该期间CPU是无法做任何其他事情的***）；

有DMA（Direct Memory Access）技术之后, I/O的过程：

<img src="assets/DRM I_O 过程.png" alt="DRM I_O 过程" style="zoom:50%;" />

通过上图，我们可以看到，DMA技术实际上就是代替CPU进行数据的搬运，而CPU不再参与任何与数据搬运相关的事情，这样CPU就会处理其他的事情。

## 3.2 `read()`和`write()`系统调用

<img src="assets/传统文件传输.png" alt="传统文件传输" style="zoom:50%;" />

期间***发生了四次用户态和内核态的上下文切换***，因为发生了两次系统调用，一次是`read()`，一次是`write()`，每一次系统调用都先从用户态切换到内核态，等待内核完成任务之后，再从内核态切换回用户态。

期间还发生了***四次数据拷贝***，两次是DMA的拷贝，另外两次则是通过CPU进行拷贝的：

- 第一次拷贝，DMA将磁盘缓冲区的内容拷贝到操作系统内核的缓冲区当中(***DMA***)；

- 第二次拷贝，CPU将内核缓冲区的数据拷贝到用户缓冲区当中，该过程是CPU执行的(***CPU***)；

- 第三次拷贝，用户将用户缓冲区的数据拷贝到内核缓冲区当中，该过程仍然是CPU执行的(***CPU***)；

- 第四次拷贝，DMA将内核的socket缓冲区的数据拷贝到网卡的缓冲区当中(***DMA***)；

我们如果想要提高文件传输的性能的话，就必须在 ***用户态和内核态切换次数***和 ***内存拷贝次数***上下文章。

1. ##### 如何减少「用户态和内核态的切换次数」？

读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。

而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。

所以，**要想减少上下文切换到次数，就要减少系统调用的次数**。

这里意思就是在整个系统当中我们最好一次性读取数据，而不是一次数据的读取分成很多次，这样就会导致大量的上下文切换。

2. 如何减少「数据拷贝次数」？

在前面我们知道了，传统的文件传输方式会历经 4 次数据拷贝，而且这里面，「从内核的读缓冲区拷贝到用户的缓冲区里，再从用户的缓冲区里拷贝到 socket 的缓冲区里」，这个过程是没有必要的。

因为文件传输的应用场景中，在用户空间我们并不会对数据「再加工」，所以数据实际上可以不用搬运到用户空间，因此**用户的缓冲区是没有必要存在的**。

3. 什么是零拷贝？

零拷贝技术就是我们没有在内存层面去拷贝数据，也就是说全程没有通过CPU来搬运数据，所有的数据都是通过DMA来进行传输的。

零拷贝技术的文件传输方式相对于传统文件传输的方式，减少了两次上下文切换和数据拷贝次数，只需要2次上下文切换和数据拷贝次数就可以完成文件的传输，而且两次数据的拷贝过程，都不需要CPU来参与。

<img src="assets/senfile-零拷贝.png" alt="senfile-零拷贝" style="zoom:50%;" />

## 3.3 Linux的I/O模型介绍以及同步异步阻塞非阻塞的区别（超级重要）

> [!IMPORTANT]
>
> 注意：同步异步和阻塞非阻塞是不相关的。同步异步强调的是，是否得到最终的结果；阻塞非阻塞强调的时间，是否等待；
>
> 同步异步区别在于调用者是否得到了想要的最终结果。同步就是一直要执行到返回最终结果；异步就是直接返回，但是返回的不是最终结果，*调用者不能通过这种调用得到结果，只需要通过被调用者的其他方式通知调用者，来取回最终结果。*
>
> 阻塞与非阻塞的区别在于，调用者是否还能干其他事情。阻塞，调用者就只能干等待；*非阻塞调用者可以先去忙别的，不用一直等。*
>
> ***注意，这里所涉及的同步还是异步是针对IO所讲述的同步和异步。***
>
> 还有一个讨论点是在业务 层面的一个逻辑处理是同步的还是异步的，就是并发的同步和异步，该讨论点就和实际项目当中的逻辑有关了，注意***并发的异步同步***和***IO的异步同步***的区别。
>
> 同步，A操作等待B操作做完事情，得到返回值，继续进行处理；
>
> 异步，A操作告诉B操作它感兴趣的事件以及通知方式，A操作继续执行自己的业务逻辑；等待B监听到相应的事件发生之后，B会通知A，A开始相应的数据处理逻辑。

https://blog.csdn.net/sqsltr/article/details/92762279

https://www.cnblogs.com/euphie/p/6376508.html

（IO过程包括两个阶段：（1）内核从IO设备读写数据和（2）进程从内核复制数据）

---

典型的一次IO的两个阶段是什么？

1. ***数据准备阶段***，就是内核从硬盘缓冲区或者其他的缓冲区当中读取数据到内核缓冲区；
2. ***数据读写阶段***，就是内核缓冲区的数据复制到用户缓冲区当中；

*数据准备阶段：*根据操作系统IO操作的就绪状态：（阻塞就是应用程序等待操作系统准备好数据进行处理；非阻塞就是可以先不等待操作系统准备好数据进行处理；）

- 阻塞I/O，(进程)CPU等待，直到数据成功复制到内核缓冲区当中；
- 非阻塞I/O，(进程)CPU不等待，直接返回去做其他的事情，之后会进行轮询查看是否准备好数据，所以说阻塞和非阻塞IO都是同步IO；

*数据读写阶段：*根据应用程序和内核的交互方式：（同步就是进程从内核复制数据的时候是阻塞的；异步就是进程从内核复制数据的时候都是不阻塞的；）

- 同步I/O，`recv()`系统调用在数据准备阶段之后，会将内核准备好的数据复制到用户缓冲区当中，如果不复制完全，进程是无法继续往下运行的，该部分就是同步的；
- 异步I/O，`aio_read()`函数调用的时候直接返回，之后利用回调机制，当数据读取完之后，调用该回调函数；

```C++
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <aio.h>
#include <fcntl.h>
#include <unistd.h>
#include <errno.h>

#define BUF_SIZE 1024

void aio_read_callback(sigval_t sigval) {
    struct aiocb *cbp = sigval.sival_ptr;
    if (aio_error(cbp) == 0) {
        printf("异步读取完成！读取的内容：%s\n", (char *)cbp->aio_buf);
    } else {
        printf("异步读取失败：%s\n", strerror(errno));
    }
    free(cbp->aio_buf);
    free(cbp);
}

int main() {
    int fd;
    struct aiocb *cbp;
    int ret;

    fd = open("example.txt", O_RDONLY);
    if (fd == -1) {
        perror("打开文件失败");
        exit(EXIT_FAILURE);
    }

    cbp = (struct aiocb *)malloc(sizeof(struct aiocb));
    if (!cbp) {
        perror("分配 aiocb 结构失败");
        exit(EXIT_FAILURE);
    }

    memset(cbp, 0, sizeof(struct aiocb));

    cbp->aio_fildes = fd;
    cbp->aio_offset = 0;
    cbp->aio_buf = (char *)malloc(BUF_SIZE);
    if (!cbp->aio_buf) {
        perror("分配缓冲区失败");
        exit(EXIT_FAILURE);
    }
    cbp->aio_nbytes = BUF_SIZE;
    cbp->aio_sigevent.sigev_notify = SIGEV_THREAD;
  	// 这里设置回调函数，当数据读取完之后进行处理
    cbp->aio_sigevent.sigev_notify_function = aio_read_callback;
    cbp->aio_sigevent.sigev_notify_attributes = NULL;
    cbp->aio_sigevent.sigev_value.sival_ptr = cbp;

    ret = aio_read(cbp);
    if (ret == -1) {
        perror("异步读取失败");
        exit(EXIT_FAILURE);
    }

    // 做一些其他工作，不会被阻塞
    printf("正在进行其他任务...\n");

    // 主动等待异步IO操作完成
    while (aio_error(cbp) == EINPROGRESS) {
        // 可以在这里做一些其他事情
    }

    close(fd);
    exit(EXIT_SUCCESS);
}
```

---

> [!IMPORTANT]
>
> 陈硕大神原话：***在处理IO的时候，阻塞和非阻塞都是同步IO，只有使用了特殊的API才是异步IO。***

* 阻塞：调用IO操作的时候，如果缓冲区空或者满了，调用的进程或者线程就会处于阻塞状态直到IO可用并完成数据拷贝。
* 非阻塞：调用IO操作的时候，内核会马上返回结果，如果IO不可用，会返回错误，这种方式下进程需要不断轮询直到IO可用为止，但是当进程从内核拷贝数据时是阻塞的。
* IO多路复用就是同时监听多个描述符，一旦某个描述符IO就绪（读就绪或者写就绪），就能够通知进程进行相应的IO操作，否则就将进程阻塞在select或者epoll语句上。


* 同步IO：同步IO模型包括阻塞IO，非阻塞IO和IO多路复用。特点就是当进程从内核复制数据的时候都是阻塞的。
* 异步IO：在检测IO是否可用和进程拷贝数据的两个阶段都是不阻塞的，进程可以做其他事情，当IO完成后内核会给进程发送一个信号。



***参考回答：***

阻塞、非阻塞和同步、异步都是用来描述I/O的一些状态。一个标准的I/O分为数据准备和数据读写两个阶段。我来拿一个很经典的系统调用`recv()`函数来举例，用户进程调用该函数的时候会通知CPU去先准备数据，就是CPU通知磁盘等硬件准备相应的数据，并且将该数据复制到内核的相应的缓冲区当中。这个过程就是准备数据，在准备数据的过程中如果说CPU也可以说是用户进程一直等待该过程，那么就是阻塞；如果不等待的话，而且执行其他的任务的话，那么就是非阻塞。数据准备之后，CPU还需要将数据拷贝到用户空间当中，该过程当中如果说用户进程一直在等待的话，就是同步；用户进程如果不等待而是采用回调函数的机制的话，就是异步I/O。`aio_read()`和`aio_write()`。

